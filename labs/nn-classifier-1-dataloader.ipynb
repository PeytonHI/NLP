{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader example\n",
    "\n",
    "This is a copy of the neural network classifier 1 with some minor changes, so if you haven't gone through that notebook, then do that first!\n",
    "\n",
    "Changes:\n",
    "- DataSet class to handle the data\n",
    "- DataLoader to support batching and shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "import evaluate\n",
    "\n",
    "import torch.nn.functional as F  # shorthand so we can do F.softmax and other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    tokenizer = Tokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "    train = []\n",
    "    with open(\"SH-TTC/train.tsv\") as fin:\n",
    "        for line in fin:\n",
    "            label, text = line.strip().split(\"\\t\")\n",
    "            tokens = tokenizer.encode(text).tokens\n",
    "            train.append((label, tokens))\n",
    "\n",
    "    dev = []\n",
    "    with open(\"SH-TTC/dev.tsv\") as fin:\n",
    "        for line in fin:\n",
    "            label, text = line.strip().split(\"\\t\")\n",
    "            tokens = tokenizer.encode(text).tokens\n",
    "            dev.append((label, tokens))\n",
    "    \n",
    "    return train, dev\n",
    "\n",
    "train_data_raw, dev_data_raw = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens):\n",
    "        self.vocab = [tok for tok, count in Counter(tokens).most_common()]\n",
    "        self.tok2idx = {tok: idx + 1 for idx, tok in enumerate(self.vocab)}\n",
    "        self.tok2idx[0] = \"[UNK]\"\n",
    "        self.idx2tok = {idx: tok for tok, idx in self.tok2idx.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tok2idx)\n",
    "    \n",
    "    def to_id(self, tok):\n",
    "        return self.tok2idx.get(tok, 0)\n",
    "\n",
    "    def to_tok(self, id):\n",
    "        return self.idx2tok.get(id, \"[UNK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab([word\n",
    "               for y, x in train_data_raw\n",
    "               for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShttcDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.data = []\n",
    "        for y, x in tokenized_data:\n",
    "            tokens = x\n",
    "            x = torch.zeros(len(vocab))\n",
    "            for tok in tokens:\n",
    "                x[vocab.to_id(tok)] += 1\n",
    "            \n",
    "            if y == \"SH\":\n",
    "                y = torch.Tensor([1, 0])\n",
    "            else:\n",
    "                y = torch.Tensor([0, 1])\n",
    "                \n",
    "            self.data.append((x, y))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ShttcDataset(train_data_raw)\n",
    "dev_data = ShttcDataset(dev_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "dev_loader = DataLoader(dev_data, batch_size=8)  # not training, so no need to shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), len(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader), len(dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the batch dimension is added on as the first dimension!\n",
    "for x, y in train_loader:\n",
    "    print(x)\n",
    "    print(x.shape)\n",
    "    print(y)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model!\n",
    "\n",
    "We are using the same model as NN clasifier 1 (just a single linear layer into 2 outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step through the model\n",
    "lin = torch.nn.Linear(len(vocab), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = lin(x)  # works with a batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(yhat, y)  # this averages the loss for the whole batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClassifier(torch.nn.Module):\n",
    "    def __init__(self, voc_size):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(voc_size, 2)  # Wx + b\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y  # don't need softmax because cross_entropy loss automatically computes it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNClassifier(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        print(name, \"\\t\", params)\n",
    "        total_params += params\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    \n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction for a single data point\n",
    "# no_grad means we don't need to calculate gradients\n",
    "# (do this when testing the model)\n",
    "with torch.no_grad():\n",
    "    x = train_data[0][0]\n",
    "    print(model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the training\n",
    "loss_func = F.cross_entropy  # same as torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train!\n",
    "for epoch in range(10):\n",
    "    print(\"Epoch\", epoch)\n",
    "    \n",
    "    for x, y in train_loader:  # use train_loader instead! Automatic batching and shuffle\n",
    "        model.zero_grad()  # do this before running\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()  # calculate gradients\n",
    "        optimizer.step()  # updates thetas\n",
    "\n",
    "    # after each epoch, check how we're doing\n",
    "    # compute avg loss over train and dev sets\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            pred = model(x)\n",
    "            loss = loss_func(pred, y)\n",
    "            total_loss += loss\n",
    "        print(\"train loss:\", total_loss / len(train_loader))\n",
    "\n",
    "        total_loss = 0\n",
    "        for x, y in dev_loader:\n",
    "            pred = model(x)\n",
    "            loss = loss_func(pred, y)\n",
    "            total_loss += loss\n",
    "        print(\"dev loss:\", total_loss / len(dev_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it is much faster to get through one epoch. Yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_on_dev_data():\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dev_data:\n",
    "            pred = model(x)  # pred is something like [0.6, 0.4]\n",
    "            preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "def sample_predictions(preds):\n",
    "    for _ in range(5):\n",
    "        idx = random.randint(0, len(dev_data))\n",
    "        \n",
    "        # argmax gives the index with the highest value\n",
    "        pred_label = \"SH\" if torch.argmax(preds[idx]) == 0 else \"TTC\"\n",
    "\n",
    "        print(\"Input:\", \" \".join(dev_data_raw[idx][1]))\n",
    "        print(\"Gold: \", dev_data_raw[idx][0])\n",
    "\n",
    "        # preds are not normalized, so for better viewing, run it through softmax\n",
    "        print(\"Pred: \", pred_label, F.softmax(preds[idx], dim=0)) \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = run_model_on_dev_data()\n",
    "sample_predictions(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate functions require numeric data, so convert labels to 0 and 1\n",
    "refs = []\n",
    "for label, text in dev_data_raw:\n",
    "    if label == \"SH\":\n",
    "        refs.append(0)\n",
    "    else:\n",
    "        refs.append(1)\n",
    "\n",
    "preds_binary = []\n",
    "for pred in preds:\n",
    "    preds_binary.append(torch.argmax(pred))\n",
    "\n",
    "print(precision.compute(references=refs, predictions=preds_binary))\n",
    "print(recall.compute(references=refs, predictions=preds_binary))\n",
    "print(accuracy.compute(references=refs, predictions=preds_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Tasks\n",
    "\n",
    "Change the batch size to see how it affects training.\n",
    "\n",
    "Then go through the NN classifier 2 notebook (with embeddings) and add a dataloader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
