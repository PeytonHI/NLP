{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In this notebook, we will build a logistic regression classifier to predict whether a sentence came from Sherlock Holmes or A Tale of Two Cities.\n",
    "\n",
    "If you don't have the packages below, run the conda command in comments. If you are running on the server and followed Winston's Remote Development Tips, then conda should be set up with conda-forge as the default channel. If not, then add `-c conda-forge` to the end of each command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1899, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/g3/888bjpv553v504qbxz78zfbw0000gn/T/ipykernel_57969/566342788.py\", line 2, in <module>\n",
      "    import torch  # conda install pytorch pytorch-cuda=12.4 -c pytorch -c nvidia\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch  # conda install pytorch pytorch-cuda=12.4 -c pytorch -c nvidia\n",
    "from tokenizers import Tokenizer  # conda install tokenizers\n",
    "from tqdm.notebook import tqdm  # conda install tqdm\n",
    "# import evaluate  # conda install evaluate scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, read the datasets, tokenize, and store as a list of (label, tokens) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    tokenizer = Tokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "    train = []\n",
    "    with open(\"SH-TTC/train.tsv\") as fin:\n",
    "        for line in fin:\n",
    "            label, text = line.strip().split(\"\\t\")\n",
    "            tokens = tokenizer.encode(text).tokens\n",
    "            train.append((label, tokens))\n",
    "\n",
    "    dev = []\n",
    "    with open(\"SH-TTC/dev.tsv\") as fin:\n",
    "        for line in fin:\n",
    "            label, text = line.strip().split(\"\\t\")\n",
    "            tokens = tokenizer.encode(text).tokens\n",
    "            dev.append((label, tokens))\n",
    "    \n",
    "    return train, dev\n",
    "\n",
    "train_data_raw, dev_data_raw = read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the data to see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SH',\n",
       " ['[CLS]',\n",
       "  '“',\n",
       "  'On',\n",
       "  'entering',\n",
       "  'the',\n",
       "  'house',\n",
       "  ',',\n",
       "  'however',\n",
       "  ',',\n",
       "  'I',\n",
       "  'examined',\n",
       "  ',',\n",
       "  'as',\n",
       "  'you',\n",
       "  'remember',\n",
       "  ',',\n",
       "  'the',\n",
       "  'si',\n",
       "  '##ll',\n",
       "  'and',\n",
       "  'framework',\n",
       "  'of',\n",
       "  'the',\n",
       "  'hall',\n",
       "  'window',\n",
       "  'with',\n",
       "  'my',\n",
       "  'lens',\n",
       "  ',',\n",
       "  'and',\n",
       "  'I',\n",
       "  'could',\n",
       "  'at',\n",
       "  'once',\n",
       "  'see',\n",
       "  'that',\n",
       "  'someone',\n",
       "  'had',\n",
       "  'passed',\n",
       "  'out',\n",
       "  '.',\n",
       "  '[SEP]'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a mapping between input features (tokens) and IDs.\n",
    "\n",
    "**Review**: Why do we build the vocabulary using only the training data and not on the dev data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_voc(data):\n",
    "    \"\"\"\n",
    "    Build vocabulary mapping, reserving idx 0 for [UNK]\n",
    "    \"\"\"\n",
    "    feat2idx = {}\n",
    "    feat2idx[\"[UNK]\"] = 0\n",
    "    next_idx = 1\n",
    "    for label, features in data:\n",
    "        for feat in features:\n",
    "            if feat not in feat2idx:\n",
    "                feat2idx[feat] = next_idx\n",
    "                next_idx += 1\n",
    "    return feat2idx\n",
    "\n",
    "feat2idx = build_voc(train_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_id(feat):\n",
    "    \"\"\"\n",
    "    Convert token to ID\n",
    "    \"\"\"\n",
    "    return feat2idx.get(feat, feat2idx[\"[UNK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_id(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9671"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOC_SIZE = len(feat2idx)\n",
    "VOC_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the raw data to numeric data (tensors in PyTorch). The input will be a vector (1-D tensor) containing counts of each feature. The output will be a single number: 0 if the sentence came from Sherlock Holmes, or 1 if the sentence came from A Tale of Two Cities.\n",
    "\n",
    "`torch.zeros(n)` creates a 1-D tensor of length n, filled with all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(raw_data):\n",
    "    \"\"\"\n",
    "    Convert data to tensors\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for label, features in raw_data:\n",
    "        # convert y to a scalar\n",
    "        if label == \"SH\":\n",
    "            y = torch.Tensor([0])\n",
    "        else:  # TTC\n",
    "            y = torch.Tensor([1])\n",
    "\n",
    "        # convert x to a vector of token counts\n",
    "        x = torch.zeros(VOC_SIZE)\n",
    "        for feat in features:\n",
    "            x[to_id(feat)] += 1\n",
    "\n",
    "        data.append((x, y))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = process_data(train_data_raw)\n",
    "dev_data = process_data(dev_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10381, 1298)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always examine your data! To check the dimensions of a tensor, use `.shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 1.,  ..., 0., 0., 0.]), tensor([0.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]  # (x, y) pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9671])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].shape  # vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][1].shape  # just one number, 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model!\n",
    "\n",
    "A logistic regression model is represented by the formula $\\sigma(Wx + b)$. `Linear` is a PyTorch object that implements $Wx + b$, and `torch.sigmoid` is the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = torch.nn.Linear(VOC_SIZE, 1)  # input = |V|, output = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0006, -0.0066, -0.0094,  ..., -0.0007,  0.0041, -0.0044]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0059], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in lin.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Which variables in the logistic regression model do the above parameters correspond to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out the different pieces of the model. First, we need an input vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an input vector\n",
    "features = [\"hello\", \"this\", \"is\", \"a\", \"test\"]\n",
    "x = torch.zeros(VOC_SIZE)\n",
    "for feat in features:\n",
    "    x[to_id(feat)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x  # a vector of token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = lin(x)  # Wx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5058], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(h)  # sigmoid(Wx + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that PyTorch automatically calculates the gradient (grad_fn), so we don't need to do this by hand. Very handy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to define the full model. When doing so, define your own class that is a subclass of torch.nn.Module. In the constructor, define any layers that contain trainable parameters (only one Linear layer, in this case).\n",
    "\n",
    "You also need to override the `forward` function to implement what happens when the model takes an input. Here, we run $x$ through the linear layer ($Wx + b$), and then pass the result into sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier(torch.nn.Module):\n",
    "    def __init__(self, voc_size):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(voc_size, 1)  # Wx + b\n",
    "    \n",
    "    def forward(self, x):     # special function called when model(x)\n",
    "        h = self.linear(x)    # h = Wx + b\n",
    "        y = torch.sigmoid(h)  # y = sigmoid(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionClassifier(VOC_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many parameters are in the model? Run the code below to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight \t 9671\n",
      "linear.bias \t 1\n",
      "Total Trainable Params: 9672\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        print(name, \"\\t\", params)\n",
    "        total_params += params\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    \n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually a tiny model, but we will see that it has surprisingly good performance. In practice, it is often better to use a simple and decently performing model than a complex model that is slightly better but requires much more resources to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When testing your model, it is a good idea to use a with guard to indicate that the model does not need to update gradients. Notice that the `pred` tensor does not store a gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5050])\n"
     ]
    }
   ],
   "source": [
    "# x is the input vector from above\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we need two things:\n",
    "- a loss function, which tells us how far off our predictions are from the correct answer (gold)\n",
    "- an optimizer, which will take the loss and update our model parameters (weights and bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.functional.binary_cross_entropy  # same as torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train! One epoch is one pass through the training data. After a pass, we will check how the model is doing by computing the average loss over the train and dev sets. Ideally, both train and dev loss will decrease. When they start to flatten out, then the model has converged, and it is a good time to stop training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf433af5a2b34ee8ae3030917bc345d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: tensor(0.6129)\n",
      "dev loss: tensor(0.6148)\n",
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d282f60e606a4c03a55b8cee08b295a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: tensor(0.5880)\n",
      "dev loss: tensor(0.5891)\n",
      "Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f0c24296514239a9538ad1069aba70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: tensor(0.5711)\n",
      "dev loss: tensor(0.5721)\n",
      "Epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c166d136ab431cb8f0a9305025359e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: tensor(0.5581)\n",
      "dev loss: tensor(0.5592)\n",
      "Epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170e23930d0243d6ac4e99c5262bdb46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: tensor(0.5478)\n",
      "dev loss: tensor(0.5492)\n",
      "Epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59b8c615aeb47aaa6f8427b9efd0f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: tensor(0.5393)\n",
      "dev loss: tensor(0.5419)\n",
      "Epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6400287fd59d4c538ffe97e41ecd8f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: tensor(0.5322)\n",
      "dev loss: tensor(0.5356)\n",
      "Epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e6112670f84c48bfd5cb1dd4c70e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: tensor(0.5259)\n",
      "dev loss: tensor(0.5314)\n",
      "Epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14fa6be8c9f24da28f626a12fb89192c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: tensor(0.5201)\n",
      "dev loss: tensor(0.5278)\n",
      "Epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd167b15e9b4414ca68262e5f479cfb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: tensor(0.5140)\n",
      "dev loss: tensor(0.5207)\n",
      "Epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28c30ebba2346a9910974e2215f56e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: tensor(0.5100)\n",
      "dev loss: tensor(0.5181)\n",
      "Epoch 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ff4f4f2c6647b7a5edb72125d50795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: tensor(0.5046)\n",
      "dev loss: tensor(0.5139)\n",
      "Epoch 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a6d4ad836a4495bab7addb91986a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: tensor(0.5005)\n",
      "dev loss: tensor(0.5106)\n",
      "Epoch 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222a47c75567490b893924bce0996aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(x)  \u001b[38;5;66;03m# run the input x through the model to get a prediction\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_func(pred, y)  \u001b[38;5;66;03m# calculate the loss between the prediction and the gold\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# calculate gradients of the loss with respect to all trainable parameters\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# update parameters based on the gradients\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# after each epoch, check how we're doing\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train!\n",
    "import random\n",
    "for epoch in range(20):\n",
    "    print(\"Epoch\", epoch)\n",
    "\n",
    "    random.shuffle(train_data)\n",
    "    for x, y in tqdm(train_data):\n",
    "        model.zero_grad()  # resets the gradients of all tensors in the model\n",
    "\n",
    "        pred = model(x)  # run the input x through the model to get a prediction\n",
    "        loss = loss_func(pred, y)  # calculate the loss between the prediction and the gold\n",
    "        loss.backward()  # calculate gradients of the loss with respect to all trainable parameters\n",
    "        optimizer.step()  # update parameters based on the gradients\n",
    "\n",
    "    # after each epoch, check how we're doing\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for x, y in train_data:\n",
    "            pred = model(x)\n",
    "            loss = loss_func(pred, y)\n",
    "            total_loss += loss\n",
    "        print(\"train loss:\", total_loss / len(train_data))\n",
    "\n",
    "        total_loss = 0\n",
    "        for x, y in dev_data:\n",
    "            pred = model(x)\n",
    "            loss = loss_func(pred, y)\n",
    "            total_loss += loss\n",
    "        print(\"dev loss:\", total_loss / len(dev_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a small model (just one neuron!), so it is fast to run through one epoch. However it takes many epochs to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our model on the dev set to get some prediction, and then evaluate the model to see how it is actually doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_on_dev_data():\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dev_data:\n",
    "            pred = model(x)\n",
    "            preds.append(pred.item())\n",
    "    return preds\n",
    "\n",
    "def sample_predictions(preds):\n",
    "    for _ in range(5):\n",
    "        idx = random.randint(0, len(dev_data))\n",
    "        pred_label = \"SH\" if preds[idx] < 0.5 else \"TTC\"\n",
    "        print(\"Input:\", \" \".join(dev_data_raw[idx][1]))\n",
    "        print(\"Gold: \", dev_data_raw[idx][0])\n",
    "        print(\"Pred: \", pred_label, preds[idx])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [CLS] Before he went away , he breathed a blessing towards it , and a [ N ##AM ##E ] . [ N ##AM ##E ] [ N ##AM ##E ] . [SEP]\n",
      "Gold:  TTC\n",
      "Pred:  SH 0.42289525270462036\n",
      "\n",
      "Input: [CLS] “ That was it , ” said [ N ##AM ##E ] , nodding app ##roving ##ly ; “ I have no doubt of it . [SEP]\n",
      "Gold:  SH\n",
      "Pred:  TTC 0.8171283006668091\n",
      "\n",
      "Input: [CLS] If he had preserved any definite re ##membrance of it , there could be no doubt that he had supposed it destroyed with the [ N ##AM ##E ] , when he had found no mention of it among the relics of prisoners which the populace had discovered there , and which had been described to all the world . [SEP]\n",
      "Gold:  TTC\n",
      "Pred:  SH 0.4512692391872406\n",
      "\n",
      "Input: [CLS] I did not breathe freely until I had taken it upstairs and locked it in the bureau of my dressing - room . [SEP]\n",
      "Gold:  SH\n",
      "Pred:  SH 0.2620030641555786\n",
      "\n",
      "Input: [CLS] “ Business seems bad ? ” [SEP]\n",
      "Gold:  TTC\n",
      "Pred:  TTC 0.522125244140625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = run_model_on_dev_data()\n",
    "sample_predictions(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate functions require numeric data, so convert labels to 0 and 1\n",
    "refs = []\n",
    "for label, text in dev_data_raw:\n",
    "    if label == \"SH\":\n",
    "        refs.append(0)\n",
    "    else:\n",
    "        refs.append(1)\n",
    "\n",
    "preds_binary = []\n",
    "for pred in preds:\n",
    "    if pred < 0.5:\n",
    "        preds_binary.append(0)\n",
    "    else:\n",
    "        preds_binary.append(1)\n",
    "\n",
    "print(precision.compute(references=refs, predictions=preds_binary))\n",
    "print(recall.compute(references=refs, predictions=preds_binary))\n",
    "print(accuracy.compute(references=refs, predictions=preds_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Tasks\n",
    "\n",
    "Improve the performance of your logistic regression classifier! Try some of the following:\n",
    "- train for more epochs\n",
    "- use a different optimizer (Adam is a good one)\n",
    "- try a different learning rate, by passing it in as an argument, e.g. (`optim = SGD(lr=0.1)`)\n",
    "\n",
    "Note: if you want to train from scratch, you need to reinitialize your model and optimizer (`model = ...`, `optim = ...`). If you rerun the training loop without reinitializing the model and optimizer, it will print epoch 0 1 2 etc but actually will have continued where it left off."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
