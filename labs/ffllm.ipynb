{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sherlock.txt\", encoding='UTF-8') as fin:\n",
    "    text = fin.read()\n",
    "    tok = Tokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    sherlock_tokens = tok.encode(text).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136135"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sherlock_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a class to handle the vocabulary will make it convenient for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens):\n",
    "        self.vocab = [tok for tok, count in Counter(tokens).most_common()]\n",
    "        self.tok2idx = {tok: idx + 1 for idx, tok in enumerate(self.vocab)}\n",
    "        self.tok2idx[0] = \"[UNK]\"\n",
    "        self.idx2tok = {idx: tok for tok, idx in self.tok2idx.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tok2idx)\n",
    "    \n",
    "    def to_id(self, tok):\n",
    "        return self.tok2idx.get(tok, 0)\n",
    "\n",
    "    def to_tok(self, id):\n",
    "        return self.idx2tok.get(id, \"[UNK]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(sherlock_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ngrams(tokens, n):\n",
    "    return [tokens[i:i + n] for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "# tokens = ngrams(sherlock_tokens, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Dataset class will let us easily batch the data with a DataLoader. You need to override two methods: `__len__` (for `len(data)`)and `__getitem__` (for `data[idx]`). We will make a bigram language model, so the input is one token and the output is also one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self, tokens):\n",
    "        self.data = []\n",
    "        for gram in ngrams(tokens, 2):\n",
    "            x = torch.LongTensor([vocab.to_id(gram[0])])  # (1)\n",
    "\n",
    "            y = torch.LongTensor([vocab.to_id(gram[1])])  # (1)\n",
    "            y = torch.nn.functional.one_hot(y, len(vocab))  # (1, V); need onehot to compute softmax\n",
    "            y = y.float().squeeze(0)  # (V); float so we can compute loss (V); squeeze removes dimensions of size 1\n",
    "            \n",
    "            self.data.append((x, y))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and dev\n",
    "train_part = int(len(sherlock_tokens) * 0.8)\n",
    "train_data = LMDataset(sherlock_tokens[:train_part])\n",
    "dev_data = LMDataset(sherlock_tokens[train_part:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5166]), tensor([0., 0., 0.,  ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.one_hot(torch.LongTensor([5]), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5167)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(train_data[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adventures'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.to_tok(5167)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataLoader lets us easily create minibatches of the data. Computing gradients on a minibatch is usually better than\n",
    "- a single point (can jump around randomly)\n",
    "- the whole data (computationally intensive)\n",
    "\n",
    "Notice that when iterating through a dataloader, the batch size is the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "dev_loader = DataLoader(dev_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5166])\n",
      "torch.Size([1])\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "torch.Size([7964])\n"
     ]
    }
   ],
   "source": [
    "for x, gold in train_data:\n",
    "    print(x)\n",
    "    print(x.shape)\n",
    "    print(gold)\n",
    "    print(gold.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task**: try removing `.squeeze(0)` when creating the dataset, then run the above cell to check the shape of the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFLLM(torch.nn.Module):\n",
    "    def __init__(self, voc_size):\n",
    "        super().__init__()\n",
    "        self.emb = torch.nn.Embedding(voc_size, 300)\n",
    "        self.linear1 = torch.nn.Linear(300, 300) \n",
    "        self.linear2 = torch.nn.Linear(300, voc_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        e = self.emb(x)\n",
    "        e = e.squeeze()  # Your task: figure out why this is necessary\n",
    "        h = self.linear1(e)\n",
    "        y = self.linear2(h)\n",
    "        return y #torch.softmax(y, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5166])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = torch.nn.Embedding(len(vocab), 300)\n",
    "linear1 = torch.nn.Linear(300, 300)\n",
    "linear2 = torch.nn.Linear(300, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 300])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = emb(x)\n",
    "print(e.shape)\n",
    "e = e.squeeze()\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = linear1(e)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7964])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = linear2(h)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7964])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = torch.softmax(y, dim=0)\n",
    "y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1338,  0.2139, -0.3422,  ..., -0.3007,  0.0190, -0.1577],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFLLM(len(vocab))\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mloss_func\u001b[49m(gold, y2)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_func' is not defined"
     ]
    }
   ],
   "source": [
    "loss_func(gold, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2977,  0.3314, -0.7737,  ...,  0.0036, -0.0572,  0.2371])\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for x, y in train_data:\n",
    "        print(model(x))\n",
    "        print(y)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cefbf5f9a59c465d8058dd9727024f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108907 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: tensor(88.0294)\n",
      "dev loss: tensor(90.5074)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    print(\"Epoch\", epoch)\n",
    "    for x, y in tqdm(train_data):\n",
    "        model.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for x, y in train_data:\n",
    "            pred = model(x)\n",
    "            loss = loss_func(pred, y)\n",
    "            total_loss += loss\n",
    "        print(\"train loss:\", total_loss / len(train_loader))\n",
    "\n",
    "        total_loss = 0\n",
    "        for x, y in dev_data:\n",
    "            pred = model(x)\n",
    "            loss = loss_func(pred, y)\n",
    "            total_loss += loss\n",
    "        print(\"dev loss:\", total_loss / len(dev_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your tasks*:\n",
    "- make this model a trigram language model (i.e. take two inputs). Look into the `torch.cat` function. You will have to modify several different parts of this notebook\n",
    "- use a data loader, which handles batching\n",
    "- you will need to modify the below function as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    result = [\"A\"]\n",
    "\n",
    "    for i in range(20):\n",
    "        idx = vocab.to_id(result[-1])\n",
    "        pred = model(torch.LongTensor([[idx]]))\n",
    "        next_idx = torch.argmax(pred).item\n",
    "        tok = vocab.to_tok(idx)\n",
    "        result.append(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
