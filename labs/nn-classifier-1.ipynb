{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Classifier 1\n",
    "\n",
    "This is a copy of the logistic regression classifier notebook with some minor changes. Ensure that you understand that notebook before this one!\n",
    "\n",
    "Changes from logistic regression:\n",
    "- Vocab class to conveniently deal with converting between features and indices\n",
    "- Instead of a single output, we have $n$ output nodes, where $n$ is the number of classes (here we have 2)\n",
    "- For the data, instead of 0 or 1 for the labels SH and TTC, we use the one-hot vectors [1, 0] and [0, 1].\n",
    "- Since we are now doing multiclass classification, the loss function is cross entropy. This loss function takes unnormalized inputs, so you do not need to manually compute sigmoid/softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "import evaluate\n",
    "\n",
    "import torch.nn.functional as F  # shorthand so we can do F.softmax and other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    tokenizer = Tokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "    train = []\n",
    "    with open(\"SH-TTC/train.tsv\") as fin:\n",
    "        for line in fin:\n",
    "            label, text = line.strip().split(\"\\t\")\n",
    "            tokens = tokenizer.encode(text).tokens\n",
    "            train.append((label, tokens))\n",
    "\n",
    "    dev = []\n",
    "    with open(\"SH-TTC/dev.tsv\") as fin:\n",
    "        for line in fin:\n",
    "            label, text = line.strip().split(\"\\t\")\n",
    "            tokens = tokenizer.encode(text).tokens\n",
    "            dev.append((label, tokens))\n",
    "    \n",
    "    return train, dev\n",
    "\n",
    "train_data_raw, dev_data_raw = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens):\n",
    "        self.vocab = [tok for tok, count in Counter(tokens).most_common()]\n",
    "        self.tok2idx = {tok: idx + 1 for idx, tok in enumerate(self.vocab)}\n",
    "        self.tok2idx[0] = \"[UNK]\"\n",
    "        self.idx2tok = {idx: tok for tok, idx in self.tok2idx.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tok2idx)\n",
    "    \n",
    "    def to_id(self, tok):\n",
    "        return self.tok2idx.get(tok, 0)\n",
    "\n",
    "    def to_tok(self, id):\n",
    "        return self.idx2tok.get(id, \"[UNK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab([word\n",
    "               for y, x in train_data_raw\n",
    "               for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_SIZE = len(vocab)\n",
    "VOC_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(raw_data):\n",
    "    \"\"\"\n",
    "    Convert data to tensors\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for label, features in raw_data:\n",
    "        # convert y label to a one-hot vector\n",
    "        if label == \"SH\":\n",
    "            y = torch.Tensor([1, 0])\n",
    "        else:  # TTC\n",
    "            y = torch.Tensor([0, 1])\n",
    "\n",
    "        # convert x to a vector of token counts\n",
    "        x = torch.zeros(VOC_SIZE)\n",
    "        for feat in features:\n",
    "            x[vocab.to_id(feat)] += 1\n",
    "\n",
    "        data.append((x, y))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = process_data(train_data_raw)\n",
    "dev_data = process_data(dev_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), len(dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model!\n",
    "\n",
    "Logistic regression has one output. To support multiclass classification, we want $n$ output nodes, where $n$ is the number of labels. For this task, $n=2$. We will use Linear again, which computes $Wx + b$. However with two outputs, there will be double the number of parameters (a W for the first output, and a separate W for the second output).\n",
    "\n",
    "Let's step through the model components before defining the actual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = torch.nn.Linear(VOC_SIZE, 2)  # input = |V|, output = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in lin.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an input vector\n",
    "features = [\"hello\", \"this\", \"is\", \"a\", \"test\"]\n",
    "x = torch.zeros(VOC_SIZE)\n",
    "for feat in features:\n",
    "    x[vocab.to_id(feat)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lin(x)  # Wx + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice there are now two outputs! For multiple outputs, we cannot use binary cross entropy. Instead, we use the regular cross entropy as the loss function. Normally, we would compute the softmax to normalize the values. However In PyTorch, `cross_entropy` automatically computes the softmax and takes the log, so there is no need to run the output through softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(pred, torch.Tensor([0, 1]))  # automatically does the softmax and log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is the model definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClassifier(torch.nn.Module):\n",
    "    def __init__(self, voc_size):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(voc_size, 2)  # Wx + b\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y  # don't need softmax because cross_entropy loss automatically computes it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNClassifier(VOC_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        print(name, \"\\t\", params)\n",
    "        total_params += params\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    \n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about twice as many parameters as the logistic regression model. More parameters = longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "# x is the input vector from above\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During test time, when we make predictions, we will use argmax to get the larger number. However, during training, we don't need to do this.\n",
    "\n",
    "Let's set up our loss function and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy  # same as torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is the same as for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    print(\"Epoch\", epoch)\n",
    "\n",
    "    random.shuffle(train_data)\n",
    "    for x, y in tqdm(train_data):\n",
    "        model.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for x, y in train_data:\n",
    "            pred = model(x)\n",
    "            loss = loss_func(pred, y)\n",
    "            total_loss += loss\n",
    "        print(\"train loss:\", total_loss / len(train_data))\n",
    "\n",
    "        total_loss = 0\n",
    "        for x, y in dev_data:\n",
    "            pred = model(x)\n",
    "            loss = loss_func(pred, y)\n",
    "            total_loss += loss\n",
    "        print(\"dev loss:\", total_loss / len(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(torch.Tensor([0.7, 0.3])) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_on_dev_data():\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dev_data:\n",
    "            pred = model(x)  # pred is something like [0.6, 0.4]\n",
    "            preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "def sample_predictions(preds):\n",
    "    for _ in range(5):\n",
    "        idx = random.randint(0, len(dev_data))\n",
    "        \n",
    "        # argmax gives the index with the highest value\n",
    "        pred_label = \"SH\" if torch.argmax(preds[idx]) == 0 else \"TTC\"\n",
    "\n",
    "        print(\"Input:\", \" \".join(dev_data_raw[idx][1]))\n",
    "        print(\"Gold: \", dev_data_raw[idx][0])\n",
    "\n",
    "        # preds are not normalized, so for better viewing, run it through softmax\n",
    "        print(\"Pred: \", pred_label, F.softmax(preds[idx], dim=0)) \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = run_model_on_dev_data()\n",
    "sample_predictions(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate functions require numeric data, so convert labels to 0 and 1\n",
    "refs = []\n",
    "for label, text in dev_data_raw:\n",
    "    if label == \"SH\":\n",
    "        refs.append(0)\n",
    "    else:\n",
    "        refs.append(1)\n",
    "\n",
    "preds_binary = []\n",
    "for pred in preds:\n",
    "    preds_binary.append(torch.argmax(pred))\n",
    "\n",
    "print(precision.compute(references=refs, predictions=preds_binary))\n",
    "print(recall.compute(references=refs, predictions=preds_binary))\n",
    "print(accuracy.compute(references=refs, predictions=preds_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Tasks\n",
    "\n",
    "Modify the model to add a hidden layer. I suggest a hidden size of 50. Make sure your dimensions match up! After running through the first Linear layer, run your output through a ReLU (`F.relu()`) to introduce non-linearity.\n",
    "\n",
    "Once you add another layer, your model training time will increase a lot per epoch because you are adding a lot more parameters. However, the number of epochs needed to get a good model will decrease, because your model is now more \"powerful\" (easily able to capture the nuances in the data). Another downside is that your model is more likely to overfit (memorize) the data. You can see this happening if your train loss gets smaller, but your validation loss increases. In this case, the best model is not the one that memorized the training data, but is the one that generalizes well to unseen validation data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
